<!DOCTYPE html>
<html>
<head>
<title> CS391 Log </title>
<link rel="stylesheet" href="style.css">
</head>
<body>

    <ul class="nav">
        <li class="nav"><a class="active" href="index.html">Home</a></li>
        <li class="nav"><a href="log.html">Log</a></li>
    </ul>

    <h2> Work Log </h2>
    <h4> Note: 9 hours must be logged per week </h4>

    <!-- week one -->
    <div>
        <h3> Week 1</h3>
        <table>
            <tr>
              <th>3 hours</th>
              <th>1 hour</th>
              <th>5 hours</th>
            </tr>
            <tr>
              <td>Worked on log webpage + reviewed HTML + CSS skills </td>
              <td>Explored Kaggle webpage features </td>
              <td>Watched videos in Coursera (How to Win a Data Science Competition), up until week 2 </td>
            </tr>
          </table>
    </div>

    <!-- week two -->
    <div>
      <h3> Week 2</h3>
      <table>
          <tr>
            <th>2 hour</th>
            <th>3 hours</th>
            <th>4 hours</th>
          </tr>
          <tr>
            <td> Anaconda Installation plus demo </td>
            <td> Coursera - How to Win a Data Science Competition videos </td>
            <td> Coursera - How to Win a Data Science Competition videos</td>
          </tr>
        </table>
  </div>

  <!-- week four -->
    <div>
      <h3> Week 4</h3>
      <table>
          <tr>
            <th> 45 minutes </th>
            <th> 15 minutes </th>
            <th>  1 hour 30 minutes </th>
            <th>  1 hour 45 minutes </th>
          </tr>
          <tr>
            <td> Spyder Tutorial. 1. Interactive Tour. 2. First steps with Spyder: Creating hello.py and executing a program, 
              calling existing functions, inspecting and updating objects. 3. Recommended first steps for Python Begineers:
              Reset the namespace. 4. Reviewed some shortcuts for Spyder. </td>
            <td> Review Getting Started with Anaconda:Launching and closing Spyder and Jupyter Notebook. 
              Creating first Jupyter Notebook. Writing Python program using Anaconda prompt. 
              Using Anaconda Prompt to launch Spyder and Jupyter Notebook.  </td>
            <td> Watched RL Course by David Silver. <a href="https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver">Click Here for source.</a> - Lecture 1: Introduction to Reinforcement Learning.
              Took notes on what is RL?, what are rewards?, Agent and environements, 3 types of states, levels of observability in environements, Policy vs value functions vs model, Exploration vs Explotation,
          Prediction vs Control. </td>
          <td> RL Course by David Silver - Lecture 2: Markov Decision Process. 
            Took notes on what is Markov Process?, Markov property, Markov Reward Process, Discounts in Markov, Bellman Equation, Markov Decision Process. </td>
          </tr>
        </table>

        <table>
          <tr>
            <th>  1 hour 30 minutes </th>
            <th>  1 hour 30 minutes </th>
            <th>  1 hour 30 minutes </th>
            <th>  1 hour 30 minutes </th>
          </tr>
          <tr>
            <td> RL Course by David Silver - Lecture 3: Planning by Dynamic Programming. 
              Took notes on:  Dynamic Programming: requirements, plans, and applications. Policy Evaluation and Iteration and Improvement. Principle of Optimality </td>
            <td> RL Course by David Silver - Lecture 4: Model-Free Prediction. 
              Took notes on: Monte-Carlo Reinforcement Learning, Monte-Carlo Policy evaluation, Temporal-difference learning, comparison between Monte-Carlo and Temporal-difference, Bootstrapping and Sampling. </td>
            <td> RL Course by David Silver - Lecture 5: Model Free Control. 
              Took notes on Model-Free Control, On and off policy learning, On-policy Monte-Carlo Control, Sarsa algorithm, On-policy Temporal-Difference Learning, Off Policy learning. </td>
            <td> RL Course by David Silver - Lecture 6: Value Function Approximation. 
              Took notes on Large-Scale reinforcement learning, value function approximations, incremental methods: gradient descent, Monte-Carlo and Temporal-difference with function approximation, batch reinforcement learning. </td>
          </tr>
        </table>
  </div>
  
  <!-- week five -->
  <div>
    <h3> Week 5 </h3>
    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 45 minutes </th>
        <th>  30 minutes </th>
        <th>  30 minutes </th>
      </tr>
      <tr>
        <td> RL Course by David Silver - Lecture 7: Policy Gradient Methods. Took notes on Value-Based and Policy-Based Reinforcement Learning. Pros and Cons of Policy-Based RL. Monte-Carlo Policy and Actor-Critic Policy Gradient. </td>
        <td> RL Course by David Silver - Lecture 8: Integrating Learning and Planning. Took notes on Model-Based Reinforcement Learning. What is a Model? Model Learning. Integerated Architectures: Dyna Architecture. Simulation-Based Search: in Monte-Carlo and Temporal-difference. </td>
        <td> Looked online for resources regarding the game snake to use as a basis for the Hungry Geese Kaggle competition.<a href="https://becominghuman.ai/designing-ai-solving-snake-with-evolution-f3dd6a9da867"> This resource </a> talked about Neural Networks and using evolution to train logic into our agent. The approach this person used is by providing random mutations to the agent so it would evolve that logic to play the snake game. </td>
        <td> Took time to read into other resources that classmates posted to shared document. Learned about a couple ways to incorporate a strategies that have already been made for the snake game into the hungry geese game. Many involved neural networks and having the agent learn from simulated experiences to gain reward. I think after reading these articles, I want to further my knowledge with Neural Networks and I see we have some sources in our common doc. </td>
      </tr>
    </table>

    <table>
      <tr>
        <th>  1 hour 45 minutes </th>
        <th> 2 hours </th>
        <th> 30 minutes </th>
        <th> 1 hour and 15 minutes </th>
        <th> 1 hour 30 minutes </th>
      </tr>

      <tr>
        <td> RL Course by David Silver - Lecture 9: Exploration and Exploitation. Took notes on Exploration vs Exploitation. Multi-Armed Bandit. Bayesain Bandits.  </td>
        <td> RL Course by David Silver - Lecture 10: Classic Games. Took notes on analysis on Classic Games: Checkers, Chess, Scarbble, etc. Perfect and Imperfect Information Games. Binary-Linear Value Function in Checkers and weighing the value of each piece. Self-play Reinforcement Learning.  </td>
        <td> Played around with the <a href= "https://bit.ly/3ccpuoS">Neural Network Playgound </a> Demo that was in the shared class document. I am still confused with how neural networks work. I see that they function like humn brains that have neurons and fluctuate their adaptability given different environments. I tried playing with finding a smaller test lost. My next step is to watch about Neural Networks and Deep Learning in coursera.  </td>
        <td> Began to watch <a href="https://www.coursera.org/learn/neural-networks-deep-learning"> Coursera </a> videos about Neural Networks and Deep Learning. I watched the first week worth of videos which introduces what the course will cover. At this point I have learned what Neural Networks are and supervised learning in them. They also touch bases on why deep learning is popular right now and its applications. </td>
        <td> Watched about half of the content for Week 2 Neural Networks and Deep Learning Coursera course. Some of the topics covered were Binary Classification, Logistic Regression, Gradient Descent, Derivatives, and Vectorization. After watching these videos I was confused and I think my next steps for next week will be to take a step back and do more application practice of basic manipulation with sets of data. I plan on going to office hours to make a plan of how to come up with my own assignments. </td>
      </tr>
    </table>
  </div>

  <!-- week six -->
  <div>
    <h3> Week 6 </h3>
    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> I looked up a follow along course on youtube to begin to pratice Python. I had previously in week 4, began to do a couple tutorials to become familiar with the Jupyter Notebooks environment. The course video can be found <a href="https://bit.ly/3sWqcgI"> here. </a> During this session I got familiarity with problem solving in Python, which has a similar format to Python. I also learned about a couple IDEs other than Jupyter Notebooks for Python. </td>
        <td> During this session I continued the course video I follow along last session. So far it is more basic review on simple declaring variables, using operations, calling %whos to get all variables that are in the workspace and getting their variable type. Pretty simple concepts that are easy to grasp because I now have experience in another language. I plan on looking for a higher level video to follow along per se more applied on data science application. </td>
        <td> Watched a very interesting <a href="https://www.youtube.com/watch?v=WXuK6gekU1Y"> documentary</a> about the game GO. This game is very popular in South Korea where they train people from a young age. The professor from the Reinforcement Learning course videos that I watched was one of the main managers in a team. This team using neural networks to develop an artificial player that would optimally find a pattern to beat any human player. The team developed their agent and had it compete against a local best player of GO in a part of Europe. Their agent is called AlphaGo. AlphaGo was able to beat this human player. Then Alpha go went up again a man who is among the top 7 world players of the game Go. With a result of 4-1, AlphGo wins. It was interesting how so many people were broadcasting these plays. Another point in their development of AlphaGo, after going against their local opponent, they had the man play several rounds with AlphaGo to find a weakness before it went against one of the worlds best players. </td>
      </tr>
    </table>

    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> I began to look into some of the notebooks provided for hungry geese <a href="https://www.kaggle.com/ihelon/hungry-geese-agents-comparison"> specifically the one for agent comparison. </a> Ricky had sugguested this resources to begin a set up environment for Hunrgy Geese during our breakout room. </td>
        <td> Picked up again on watching <a href="https://www.coursera.org/learn/neural-networks-deep-learning"> Coursera </a> videos about Neural Networks and Deep Learning. I stoped at about a quarter of week 3 material. Some topics that were covered were more examples of vectorization, broadcasting Python and Jupyter Notebooks (which was more of a recap), representing Neural Networks and what computing their outputs looks like.   </td>
        <td> Continued the Neural Networks and Deep Learning Coursera, wrapped up week 3 topics. Some of the topics that were highlighted were more Vectorization examples, activation function which calculate and determine the output of neural networks, derivatives of activation functions. </td>
        <td> I followed along with a <a href="https://www.kaggle.com/rutailiu/titanic-ml-project">journal</a> from a team for the Titanic Kaggle Competition. I needed some example of importing data and manipulating it, analyzing it, and manipulating it. </td>
      </tr>
    </table>
  </div>

  <!-- week seven -->
  <div>
    <h3> Week 7 </h3>
    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> My intent during this seating was to set an environement of Pyhton and Juptyer Notebook in VSCode, but after a while I decided that I would just use the environments I have. I could not get a Python terminal, like Ricky had mentioned that we should have, instead I had a powershell console. I also browsed through the DS256 course page and material. I set a space on my laptop to install the DS Jupyter Notebooks. I plan on doing some of those assignments in the future to begin to learn the applied coding aspect in DS to prepare for the Exam. </td>
        <td> After looking at some of the coding implementation for more DS focused, I reevaluated my confidence with Python. In a previous week I followed along with a begineers Youtube video to Python and because I began with variable declarations, assignment, and basic operators, it seemed easy to me. However, during this session I went back to Python learning. This time around I followed the <i>Whirlwind Tour of Python</i> by Jake VanderPlas. I started with Basic Python Semantics: Operators. Although I had reviewed operators before, I went to the end of the section to look at <i>Identity and Membership Operators</i> because I have never heard of that before. I ended at <i>Built-In Types: Simple Values</i> which was about variable types. Something I found interesting is how Python has the type "NoneType" that indicates null, meanwhile, I belive in Java you just store null in any data type straight forward. </td>
      </tr>
    </table>

    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> During this session I continued <i>Whirlwind Tour of Python</i> by Jake VanderPlas. I started with the <i>Built-In Data Structures.</i> I don't know how much I will need apply data structures in the coding for the DS competitions. However, it was nice to review some concepts since I took the course last year. The data structures covered were list, tuple, dict(hashmaps), and set. This included declaring, using pertaining methods, and accessing values. I ended on <i>Control Flow</i> which covered condition statements and loops. I decided to review this seccion because although I know how loops and condition statements work, I wanted to understand the syntax of these funtions. </td>
        <td> This session was a continuation of <i>Whirlwind Tour of Python</i> by Jake VanderPlas. This time I reviewed <i>Defining and Using Functions.</i> Again this was a quick review to look for syntax in Pyhton just like I did for conditional statements and for loops in my last session. This section also covered Flexible Arguements and Anonymous Functions, which was new to me. Then I ended in <i>Errors and Excpetions.</i> This was a review of different types of errors and writing code to catch those exceptions. Also the command raise was covered, which reminds me of when we use print statements to debug in Java.</td>
      </tr>
    </table>

    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> Continued <i>Whirlwind Tour of Python</i> by Jake VanderPlas in during this session. I began with <i>Iterators</i>. This section simply covered implementing a range and a for-each in loops. Enumurate to keep track of the index. Zip which syncs two iterables when you want to iterate multple lists at the same time. Map which combines an iterator and function over a list. Filter is the same as Map, but it only shows values that are true for the function. </td>
        <td> During this section I started with <i>List Comprehension</i> in <i>Whirlwind Tour of Python</i> by Jake VanderPlas. This section is rewriting loops in a format that would match the English more. I ended at <i>Generators</i>, which is still confusing to me. I understood that it is similar to List Comprehension. The book described it as a recipe for producing values, however, I still do not understand the difference between these two concepts. </td>
        <td> I continued <i>Whirlwind Tour of Python</i> by Jake VanderPlas, starting with <i>Modules and Packages.</i> This had to do with importing modules and this also includes third-party modules. This makes sense how we can import modules from lets say Kaggle and make working with data science. I also went over <i>String Manipulation and Regular Expressions.</i> This section was very lengthy and was nice to go over string specific methods in Python, so it was straightforward. Next week I hope to begin chapter 5 from <i>Python Data Science Handbook</i> by Jake VanderPlas. I want to focus on chapter 5 because it is about machine learning and basic In-Depth algorithms that I could potentially use for the exam. </td>
      </tr>
    </table>
  </div>

   <!-- week eight -->
   <div>
    <h3> Week 8 </h3>
    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> During this session I followed along the Preview of Data Science Tools section from <i>Whirlwind Tour of Python</i> by Jake VanderPlas. This was an introduction to some of the packages and basic commands possible in those packages. I followed along with the textbook by making my own problems in a Jupyter Notebook. The packages covered were Numpy which allows us to have multi-dimensional arrays. Pandas which builds off of Numpy and allows to apply labels to the arrays. Matplotlib is a visualization package to see plots. SciPy is more of a numerical computing package. This was nice to have to ease into the next Jake VanderPlas Python book. </td>
        <td> I began the <i>Python Data Science Handbook</i> by Jake VanderPlas. I started at the IPython: Beyind Normal Python. In this section I was able to follow along with <i>Help and Documentation in Python</i> through <i>Input and Output History</i> in my own separate Jupyter Notebook. These sections were introduction to using functions and shortcuts in IPython. I decided to use the Jupyter Notebooks in a URL instead of the command line. Although this was somewhat of a review I decided to take your input and begin from the start. This way I can build up to chapter 5, which will help me out with the exam.  </td>
        <td> This session I completed the other half of the <i>IPython: Beyond Normal Python</i> section in the <i>Python Data Science Handbook</i> by Jake VanderPlas. I started at <i>IPython and Shell Commands</i> and ended at <i>More IPython Resources.</i> This section covered shell commands which were commands we learned in cs111. How to debug in my code using the %xmode command, which had three possibilities. I liked how the plain option gives you a condense version of your error because it can be tough to depict what the error is. I also learned about commands to time code snippets. </td>
      </tr>
    </table>

    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> During this session I started the <i>Introduction to NumPy</i> in the <i>Python Data Science Handbook</i> by Jake VanderPlas. I got through <i>Understanding Data Types in Python</i> up to <i>Computation on NumPy Arrays: Universal Functions.</i> I learned more about data types in NumPy, specifically Integers and the many functions to Arrays. Arrays seems to be an ongoing important data structure within data science. I also learned about ufuncs which are used to execute repeated operations much more quickly than loops. </td>
        <td> I continued the <i>Introduction to NumPy</i> in the <i>Python Data Science Handbook</i> by Jake VanderPlas. I followed along in my own Jupyter Notebook, starting at <i>Aggregations:Min, Max, and Everything in Between</i> to <i>Comparisons, Masks, and Boolean Logic.</i> Some things I learned were basic operations like summing values in an array, finding the min and max. It is important to know how to do these basic operations to manipulate the data sets. Broadcasting was also brough up where operations can be performed on all values no matter what the array dimensions are. One example was adding a scalar to the array. In the last section that I followed along, they talked about comparison operators and working with boolean operators and arrays.</td>
        <td> I finished the the <i>Introduction to NumPy</i> in the <i>Python Data Science Handbook</i> by Jake VanderPlas section. During this session I learned about exploring fancy indexing, which means to pass an array of indices to access various elements at the same time. It also talked about sorting data in arrays, which it brought up algorithms that we have seen in 216. The last part was about structured arrays which are arrays with compound data types to indicate that there is a relationship between them. We could separate the data into different arrays based on data type, but we wouldn't know if they relate at all. </td>
      </tr>
    </table>
  </div>

  <!-- week nine -->
  <div>
    <h3> Week 9 </h3>
    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> This week I intend on going through chapter 3 (Pandas) of <i>Python Data Science Handbook</i> by Jake VanderPlas textbook. During this session I got through <i>Introducing Pandas Objects</i> and finished at <i>Operating on Data in Pandas.</i> Pandas are a built off of NumPy to enhance the data structures and understand them mor thoroughly. I followed along on my own on my Jupyter Notebook. The Pandas introduction seems very similar to NumPy so far, with just an introduction of some new functions.</td>
        <td> I picked up on chapter 3 of the <i>Python Data Science Handbook</i> by Jake VanderPlas textbook. I started at <i>Handling Missing Data</i> and ended at <i>Combining Datasets: Concat and Append.</i> It was interesting to read bout missing data. It was stated that in real life sometimes we don't have a clean cut  set of data. Therefore, in Pandas, there are multiple ways to indicate these missing data bits here and there. I wonder if when using agents, eventually, these missing data bits get filled up by some value because the agents are making estimations based on future states and environments. Another neat take away was MultiIndex in Pandas. It's very neat how we can organize and priotitize more certain values in arrays. </td>
        <td> I continued with the Jake VanderPlas, and I will state now that my entire week of work is intended to be from the Python Data Science textbook. So I picked up on <i>Combining Datasets: Merge and Join</i> and followed along and worked through <i>Pivot Tables.</i> During this section it was nice to know that it is Pandas allows an easy way to merge data. There are also many ways to megre data: one-to-one, many-to-one, and many-to-many. There are also many ways to specific how to nicely line the data by columns. Overall, these sections had to do with grouping data in various manners. </td>
      </tr>
    </table>

    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> I covered sections <i>Vectorized String Operations</i> to <i>High-Performance Pandas: eval() and query()</i> in chapter 3. One great thing about Panda is vectorization of operations, which makes manipulating arrays so much easier, without having to worry about size. I also learned about times and datas in Panda and how it is useful that there are many functions to easily organize tables of data with dates. This section was a little tougher to understand only because there was a lot of information that was thrown as a reader. It is kind of overwhelming learning all of these functions. </td>
        <td> During this session, I started chapter 4, <i>Visualization with Matplotlib.</i> I got through the introduction of this chapter and ended at <i>Density and Contour Plots.</i> So Matplotlib is a visualization library for data. There was not much that stuck out to me. It reminded me of my mathematica labs that I've done for multivariable so far. Using commands to show graphs and visualization of trends. </td>
        <td> I continued with chapter 4 of <i>Python Data Science Handbook</i> by Jake VanderPlas. I started at <i>Histograms, Binnings, and Density</i> and finsihed at <i>Multiple Subplots.</i> I felt again during this session that there wasn't anything too crazy presented. I definitely think is it important to have these tools to visualize, but again it reminded me of my mathematica lab to an extent. I think I may just skip to the start of chapter 5 next week to finally start to implement some sort of basic data technique implementation into my exam because I know chapter 5 introduces them. </td>
      </tr>
    </table>
  </div>

  <!-- week ten -->
  <div>
    <h3> Week 10 </h3>
    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  2 hours </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> I worked with Kelvin during this time slot. Kelvin expressed how he was having a tough time with the work logs. We talked about some strategies and tips to be consistent with work. We also sugguested some pointers when entering entries. I mentioned that reflecting about what you looked at for you work helps yourself and you(Professor Neller) to know where to go next. Whether that be to take a step back or continue with the current topic. After that we also discussed some material we had covered in our logs and resources. We exchanged some resources to look at next to plan things that we have to learn more about. I expressed that I needed help with begining to do exercises that manipulate data. Therefore, Kelvin sugguested the Kaggle <a href="https://www.kaggle.com/learn/intro-to-machine-learning">Intro to Machine learning.</a> Lastly, we discussed how we both had not submitted an entry for hungry geese yet, so we talked about looking into the discussion tab in the compeition to find some starter code. </td>
        <td> During this session my intension is to go through <a href="https://www.kaggle.com/learn/intro-to-machine-learning">Intro to Machine Learning</a> on Kaggle. I was able to sucessfully, read through all 7 lessons. As for exercises I completed almost 3/6. This course helped me understand hwo to set up a prediction target(y) and prediction feautured you would like to use(X.) I was able to start building my first model, which is a decision tree. I also was able to implement and define a DecisionTreeRegressor. I learned how to fit your train data into the model and use the method .predict(). However, I do remember that you want us to use .predict_proba() because it can affect loss. Overall, the course uses an example of making predictions for prices of new houses based on the ones seen before. This clarified how to asses what your target should be. I also read that your x, which are the features used to predict, can be all the rest of the columns minus y or specific choosen ones. </td>
        <td> During this session I began to collect some resources to begin my exam. I intended on researching some basic models which would fit best for the exam. I began by <a href="https://www.javatpoint.com/regression-vs-classification-in-machine-learning">understanding classification and regression</a> in machine learning which I had read from previous sessions that works best for labeled datasets. Then I looked in regression models and the model I came accross was Logistic Regression. This would be a great start because it is able to determine the probability of an output based on categories. <a href="https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc">in this resource,</a> it explained how it can be used on an example, such as determining the probability of a data output being spam or not. This can be applicable to our exam problem because we want to determine the probability of a output being in the range [0,1], more specifically 1. After I found the model I would use, I continued to look on <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">scikit learn</a> to study the atributes of declaring a Logistic Regression model and some of it's methods. </td>
      </tr>
    </table>

    <table>
      <tr>
        <th>  2 hours </th>
        <th>  2 hours </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> During this session I intended to do one through to set up my jupyter notebook for the exam and import the exam data. I began by first importing the data and saving it in the same folder where my exam Jupyter Notebook is located. I then began to import some libraries. After that I established that my target and features, aka: X and y. I determinded that our y is just the y column and our features X would be the rest of our columns as read in a previous source. In my second session of the week, where the reading explained that you can set your features to be every column minus the target prediction. I don't know how much to go in depth with the following journal entries because I believe I will spend most of my time on the exam this week. I intend on expressing roadbloack from here. In this session I wrote code from the set up of x and y all the way to using predict_proba() on my model. I have not ran my cells set because I wanted to do a run through. I will be running my notebook next session. </td>
        <td> This session I ran each cell in my notebook. So far I have an error in the cell where I create and fit my model. I received a error in my cell that contains the code of the model set up and calling methods on the model. The error says to increase the number of iterations because it cannot converge. I looked back at the Logistic Regression API on the scikit website I mentioned in a previous entry. Here I found what max_iter meant. It is an attribute that can be specified as a parameter when defining the Logistic Regression model. The default iterations for this model is 140 I believe from what I found in the API, however, max_iter allows you to specify how many iterations it should make. I then ran this cell and lets just say it took a hour to finish running and my laptop was overheating! I did take a 20 minute break in this session that I did not count into the total time. Meanwhile, my code ran I did try to look at why my code was taking long, because I knew it meant something was wrong. After the cell finishing running, I received the same error, which was very frustrating. From here I was not sure where to go next, so I stopped for this session. I had found while my cell ran, that it could have to do with my previous set up before the model or having to resize my data. As for resizing my data, I wasn't sure if this would be allowed, so I do not intend on doing this. </td>
        <td> During this session I picked up back on my exam, this is the same day as the last entry. I attempted to review my set up previous to declaring my model. At this point I had received your email about tips for the exam. I quickly realized that I had set my y and X to the incorrect value(s). I had initially set the y (predict target) to the column 'x1' thinking that it would get me the 1 values in the data. As for my X, I had set the features to be 'x2' to 'x40'. I looked back at the train data table and I saw that there was a 'y' column. I had completely missed this! Needless to say, I laughed at myself for missing it and it took me quite a while to notice this. So I went back and reset my y and X values. I adjusted the X to include 'x1' as well. At this point I have gotten to use the .predict_proba() method on my model to save it into a variable. I intend on looking into how to write my results onto a .csv file. </td>
      </tr>
    </table>

    <table>
      <tr>
        <th>  2 hours </th>
        <th>  1 hour </th>
      </tr>
      <tr>
        <td> I picked up again where I left off on the exam. Let me just say, I know we should not take as much time as I reached now, but I feel like most of my time was spent trouble shooting. During this session, like I said in my previous entry I am going to look into how to create a csv file and write my ouput for 1. This took a bit to figure out. This might have been the hardest part to figure out! The <a href="https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html">.read_csv</a> API link that you shared helped a lot. I had to break down the examples provided at the end and what each parameter meant. Before that I did look into the <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html">DataFrame API.</a> I looked into this to create a data frame that would contain column names for 'id' and 'y' and just the y = 1 values. I had to find a way to get the range which also took a while to figure out, but then I tried to approach it in a similar way to how we set the range of an array that we want because essentially our predict variable is an array. Then I implemented the .to_csv on my DataFrame and create a name for the file. As well as set the index to false so that the row names would not show up from the dataframe. I then submitted and I was 0.00018 away from the benchmark. Here I am taking a break again. </td>
        <td> Here I picked up on my first draft and submission. I wanted to improve my loss. So I looked into the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">Logistic Regression</a> to find other parameters that could help me similar to when I modified the max_iterations. I did end up seeting the random_state to 0 because it is something I saw a pattern of in many examples I had seen that defined the Logistic Regression model. After time I looked into the solver parameter and tested different values. I found that the 'sag' improved my predictions to be lower and I think it has to do with the resource saying that this value is best to optimize speed with larger data sets. I compared my numbers from my first entry and I decided to submit. I hit the benchmark! Let's say I was very excited after being stressed and I could not fall asleep because I was so excited. I still plan on going to office hours Monday to see if I can maybe decrease my loss someway.  </td>
      </tr>
    </table>
  </div>

  <!-- week eleven -->
  <div>
    <h3> Week 11 </h3>
    <table>
      <tr>
        <th>  3 hours </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> During this session I worked more on the exam. I intend on improving my loss for the exam. I have hit the benchmark for the exam, but it would be nice to improve. I tested a couple of other models that I had research from classification and regression. I tested the Decision Tree and Random Forest models and Bayesian model. With the tree models I noticed that it would cause there to be a lot of missing data. I tried playing around with the parameters of these models. However, I was not able to find a solution to get the missing data. I also tried Bayesian model which did cause some of the predictions to be lower than my logistic regression. However, in others lines it was way higher than in my logistic regression. I did submit my Bayesian model output predictions on Kaggle, but my loss ended up being higher. At this point I ended up giving up because I wasn't sure how to improve the log loss any further in the time constraint I had and the number of submissions. </td>
        <td> In this session I returned back to the <i>Python Data Science Handbook</i> by Jake VanderPlas. I decided to skill chapter 4 because I don't see the point to learning about Matplotlib at this moment. I also wanted to begin chapter 5 <i>machine learning</i> which I was meaning to get to a while ago, but I wanted to get the foundation of Numpy and Pandas. I got through <i>What is Machine Learning?</i> to <i>Feature Engineering.</i> This section was more a review of what I had learned last week with what machine learning is and the functions and imports you can use from Scikit-learn. This section I did not do much application, only reading, only because I did not see it necessary to do application since I had familiarized myself while I worked on the exam. </td>
      </tr>
    </table>

    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
      </tr>
      <tr>
        <td> I intend to continue chapter 5 from the <i>Python Data Science Handbook</i> by Jake VanderPlas. I picked up on the models in this chapter. I tried the Naive Bayes Classification, Linear Regression, and Support Vector Machines. I followed along with the text book and actually applied it to the data set that was provided for the exam since I already had it imported. I had tried Naive Bayes and Linear Regression during the exam, so it was more review. However, the Support Vector Machines was new and it mentioned how it was helpful for classification problems. My favorite part was when in the Support Vector Machines section, they provided an example of where you would apply this model. The example was to use it on a facial recognition problem. Although I did not get to test this model on the data for the exam, I do want to eventually get to this model to try a facial recognition problem. They provided a list of presidents in the book and its fascinating as I go on in the book of what you can use these models for. </td>
        <td> In this session I tried three more models from the Python book. I tried the Decision Trees, Random Forests, and Manifold Learning. I remember trying to use the Decision Trees model in the exam and I had missing data and the same for Random Forests. I still got missing values. I do enjoy the decision trees because when they provided in an example in the book, I found it the easiest to understand since it is a tree similar to those puzzles where you have two paths and based on the choices you make, it leads to one output at the end. </td>
        <td> During this session I finished the rewiew of "In Depth" models in chapter 5. I went through k-Means Clustering, Gaussian Mixture Models, and Kernel Density Estimation. These models seemed to be more memory intensive. I did get very confused when I was trying to implement the models myself. I do think I have to revisit these concepts once more because they were more elaborate than the first models that are in this chapter. </td>
      </tr>
    </table>
  </div>

</body>
</html>