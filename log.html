<!DOCTYPE html>
<html>
<head>
<title> CS391 Log </title>
<link rel="stylesheet" href="style.css">
</head>
<body>

    <ul class="nav">
        <li class="nav"><a class="active" href="index.html">Home</a></li>
        <li class="nav"><a href="log.html">Log</a></li>
    </ul>

    <h2> Work Log </h2>
    <h4> Note: 9 hours must be logged per week </h4>

    <!-- week one -->
    <div>
        <h3> Week 1</h3>
        <table>
            <tr>
              <th>3 hours</th>
              <th>1 hour</th>
              <th>5 hours</th>
            </tr>
            <tr>
              <td>Worked on log webpage + reviewed HTML + CSS skills </td>
              <td>Explored Kaggle webpage features </td>
              <td>Watched videos in Coursera (How to Win a Data Science Competition), up until week 2 </td>
            </tr>
          </table>
    </div>

    <!-- week two -->
    <div>
      <h3> Week 2</h3>
      <table>
          <tr>
            <th>2 hour</th>
            <th>3 hours</th>
            <th>4 hours</th>
          </tr>
          <tr>
            <td> Anaconda Installation plus demo </td>
            <td>Python Coding Review </td>
            <td>Coursera - How to Win a Data Science Competition videos</td>
          </tr>
        </table>
  </div>

  <!-- week four -->
    <div>
      <h3> Week 4</h3>
      <table>
          <tr>
            <th> 45 minutes </th>
            <th> 15 minutes </th>
            <th>  1 hour 30 minutes </th>
            <th>  1 hour 45 minutes </th>
          </tr>
          <tr>
            <td> Spyder Tutorial. 1. Interactive Tour. 2. First steps with Spyder: Creating hello.py and executing a program, 
              calling existing functions, inspecting and updating objects. 3. Recommended first steps for Python Begineers:
              Reset the namespace. 4. Reviewed some shortcuts for Spyder. </td>
            <td> Review Getting Started with Anaconda:Launching and closing Spyder and Jupyter Notebook. 
              Creating first Jupyter Notebook. Writing Python program using Anaconda prompt. 
              Using Anaconda Prompt to launch Spyder and Jupyter Notebook.  </td>
            <td> Watched RL Course by David Silver. <a href="https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver">Click Here for source.</a> - Lecture 1: Introduction to Reinforcement Learning.
              Took notes on what is RL?, what are rewards?, Agent and environements, 3 types of states, levels of observability in environements, Policy vs value functions vs model, Exploration vs Explotation,
          Prediction vs Control. </td>
          <td> RL Course by David Silver - Lecture 2: Markov Decision Process. 
            Took notes on what is Markov Process?, Markov property, Markov Reward Process, Discounts in Markov, Bellman Equation, Markov Decision Process. </td>
          </tr>
        </table>

        <table>
          <tr>
            <th>  1 hour 30 minutes </th>
            <th>  1 hour 30 minutes </th>
            <th>  1 hour 30 minutes </th>
            <th>  1 hour 30 minutes </th>
          </tr>
          <tr>
            <td> RL Course by David Silver - Lecture 3: Planning by Dynamic Programming. 
              Took notes on:  Dynamic Programming: requirements, plans, and applications. Policy Evaluation and Iteration and Improvement. Principle of Optimality </td>
            <td> RL Course by David Silver - Lecture 4: Model-Free Prediction. 
              Took notes on: Monte-Carlo Reinforcement Learning, Monte-Carlo Policy evaluation, Temporal-difference learning, comparison between Monte-Carlo and Temporal-difference, Bootstrapping and Sampling. </td>
            <td> RL Course by David Silver - Lecture 5: Model Free Control. 
              Took notes on Model-Free Control, On and off policy learning, On-policy Monte-Carlo Control, Sarsa algorithm, On-policy Temporal-Difference Learning, Off Policy learning. </td>
            <td> RL Course by David Silver - Lecture 6: Value Function Approximation. 
              Took notes on Large-Scale reinforcement learning, value function approximations, incremental methods: gradient descent, Monte-Carlo and Temporal-difference with function approximation, batch reinforcement learning. </td>
          </tr>
        </table>
  </div>
  
  <!-- week five -->
  <div>
    <h3> Week 5 </h3>
    <table>
      <tr>
        <th>  1 hour 30 minutes </th>
        <th>  1 hour 30 minutes </th>
        <th>  30 minutes </th>
        <th>  1 hour 30 minutes </th>
        <th>  2 hours </th>
      </tr>
      <tr>
        <td> RL Course by David Silver - Lecture 7: Policy Gradient Methods. Took notes on ... </td>
        <td> RL Course by David Silver - Lecture 8: Integrating Learning and Planning. Took notes on ... </td>
        <td> Looked online for resources regarding the game snake to use as a basis for the Hungry Geese Kaggle competition. This resource talked about Neural Networks and using evolution to train logic into our agent. The approach this person used is by providing random mutations to the agent so it would evolve that logic to play the snake game. </td>
        <td> RL Course by David Silver - Lecture 9: Exploration and Exploitation. Took notes on ... </td>
        <td> RL Course by David Silver - Lecture 10: Classic Games. Took notes on ... </td>
      </tr>
    </table>
  </div>

</body>
</html>